{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dY9EzB3qDLY",
        "outputId": "2a391e51-3df5-486c-ab3f-631430ca6241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa_Q9CqvpvtV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # SwiGLU: Swish(xW1) \u2299 (xW3) W2\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # More numerically stable RMSNorm\n",
        "        # Calculate mean of squares\n",
        "        mean_square = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "        # RMS normalization\n",
        "        rms = torch.sqrt(mean_square + self.eps)\n",
        "        return self.weight * x / rms\n",
        "\n",
        "class FnetBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.rmsnorm1 = RMSNorm(embed_dim)\n",
        "\n",
        "        self.rmsnorm2 = RMSNorm(embed_dim)\n",
        "\n",
        "        # self.mlp = nn.Sequential(\n",
        "        #     nn.Linear(embed_dim, embed_dim*2),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(embed_dim*2, embed_dim*2),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(embed_dim*2, embed_dim)\n",
        "        # )\n",
        "\n",
        "        self.mlp = SwiGLU(embed_dim, embed_dim*4)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = x + torch.fft.fft(self.rmsnorm1(x), dim=1).real\n",
        "        out = out + self.mlp(self.rmsnorm2((out)))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FNET(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, embed_dim, context_length, vocab_size, num_layers=2, lr=0.0001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lr = lr\n",
        "\n",
        "        self.context_length = context_length\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        self.pos_embeddings = nn.Embedding(context_length, embed_dim)\n",
        "\n",
        "        self.blocks = nn.ModuleList([FnetBlock(embed_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.norm = RMSNorm(embed_dim)\n",
        "\n",
        "        self.output = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "        # print(f\"self.output.weight.shape: {self.output.weight.shape}\")\n",
        "\n",
        "        # print(f\"self.word_embeddings.weight.shape: {self.word_embeddings.weight.shape}\")\n",
        "\n",
        "        self.loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.output.weight = self.word_embeddings.weight\n",
        "\n",
        "    def forward(self, input_ids, attention_mask:Optional[torch.tensor]=None):\n",
        "\n",
        "        embs = self.word_embeddings(input_ids) + self.pos_embeddings(torch.arange(0, self.context_length).to(input_ids.device))\n",
        "\n",
        "        if attention_mask:\n",
        "            attention_mask = torch.tril(torch.ones((self.context_length, self.context_length), device=input_ids.device))\n",
        "            mask = attention_mask.unsqueeze(-1).expand_as(embs)\n",
        "            embs = embs*mask\n",
        "\n",
        "        for layer in self.blocks:\n",
        "            embs = layer(embs)\n",
        "\n",
        "        embs = self.norm(embs)\n",
        "\n",
        "        logits = self.output(embs)\n",
        "        return logits\n",
        "    # Fix 1: Update training_step to reshape logits and targets\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch[0], batch[1]\n",
        "\n",
        "        # print(x.shape, y.shape)\n",
        "\n",
        "        out = self(x)  # Shape: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        # Reshape for CrossEntropyLoss\n",
        "        loss = self.loss_func(out.view(-1, out.size(-1)), y.view(-1))\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch[0], batch[1]\n",
        "\n",
        "        out = self(x)\n",
        "\n",
        "        # Reshape for CrossEntropyLoss\n",
        "        loss = self.loss_func(out.view(-1, out.size(-1)), y.view(-1))\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr = self.lr)\n",
        "\n",
        "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        #     optimizer,\n",
        "        #     mode='min'\n",
        "        # )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "        }\n",
        "\n",
        "\n",
        "    # def configure_gradient_clipping(self, optimizer, gradient_clip_val = None, gradient_clip_algorithm = None):\n",
        "    #     self.clip_gradients(\n",
        "    #         optimizer,\n",
        "    #         gradient_clip_val=1.0,\n",
        "    #         gradient_clip_algorithm='norm'\n",
        "    #     )\n",
        "\n",
        "\n",
        "class TokenDataset(Dataset):\n",
        "   def __init__(self, tokens_path=\"Tokens.pt\", max_length=256):\n",
        "       self.tokens = torch.load(tokens_path, weights_only=True)\n",
        "       self.tokens = self.tokens[:103218]\n",
        "       self.max_length = max_length\n",
        "      #  print(\"here\")\n",
        "   def __len__(self):\n",
        "       return len(self.tokens) - self.max_length\n",
        "\n",
        "   def __getitem__(self, idx):\n",
        "       x = self.tokens[idx:idx + self.max_length]\n",
        "       y = self.tokens[idx + 1:idx + self.max_length + 1]\n",
        "       return x, y\n",
        "\n",
        "\n",
        "def create_dataloader(tokens_path=\"Tokens.pt\", batch_size=16, max_length=512, num_workers=0):\n",
        "    dataset = TokenDataset(tokens_path, max_length)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_model():\n",
        "\n",
        "    embed_dim, context_length, vocab_size = 512, 256, 30002\n",
        "    checkpoint_path = \"/content/lightning_logs/version_7/checkpoints/epoch=3-step=6436.ckpt\"\n",
        "    model = FNET.load_from_checkpoint(checkpoint_path, embed_dim=embed_dim, context_length=context_length, vocab_size=vocab_size, num_layers=4)\n",
        "    # model = FNET(embed_dim, context_length, vocab_size, num_layers=4)\n",
        "\n",
        "    train_loader = create_dataloader(tokens_path=\"Tokens.pt\", batch_size=64, max_length=256)\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=100,\n",
        "        enable_progress_bar=True,\n",
        "        num_nodes=1,\n",
        "        enable_checkpointing=True,\n",
        "        gradient_clip_val=1.0,\n",
        "        gradient_clip_algorithm='norm'\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_dataloaders=train_loader)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     x = torch.randn((1, 5, 512))\n",
        "\n",
        "#     block = FnetBlock(512)\n",
        "\n",
        "#     out = block(x)\n",
        "#     print(out.shape)\n",
        "\n",
        "#     input_ids = torch.randint(0, 20002, (1, 10))\n",
        "\n",
        "#     model = FNET(512, 10, 20002)\n",
        "\n",
        "#     out = model(input_ids)\n",
        "#     print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIs_qcbL1_aV",
        "outputId": "80d9beb7-24b2-4607-8817-ef3dbafc7eaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4240059])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.load(\"Tokens_small.pt\").shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451,
          "referenced_widgets": [
            "235a1e2d54084cf99558160b5bebf439",
            "1bde9644a0474bb089113e7e8f9aef63",
            "8e643d826eb546388f42ee1e28a64aea",
            "b5836825ebc34eff849363d2ba760ab6",
            "a9bb31bf7f5545f8b407cac2e8354a9f",
            "1efc27b5e21a43b981ba92b1e04f4aad",
            "c906a466b728468685b96e900f7a41da",
            "f58b570e3b2040fbbcb5eea69c9e1b90",
            "e00f74cf7b81426b84f7fd311f01f497",
            "56c61ac407d341cf9143e5bcd4e3d2a7",
            "f80c4e92fbde4edc9e1bdaaf50ccc099"
          ]
        },
        "id": "lhAkrJ--p52q",
        "outputId": "89121913-bd12-40ac-b3e3-d2771fdb04c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:\ud83d\udca1 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name            | Type             | Params | Mode \n",
            "-------------------------------------------------------------\n",
            "0 | word_embeddings | Embedding        | 15.4 M | train\n",
            "1 | pos_embeddings  | Embedding        | 131 K  | train\n",
            "2 | blocks          | ModuleList       | 12.6 M | train\n",
            "3 | norm            | RMSNorm          | 512    | train\n",
            "4 | output          | Linear           | 15.4 M | train\n",
            "5 | loss_func       | CrossEntropyLoss | 0      | train\n",
            "-------------------------------------------------------------\n",
            "28.1 M    Trainable params\n",
            "0         Non-trainable params\n",
            "28.1 M    Total params\n",
            "112.318   Total estimated model params size (MB)\n",
            "34        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "235a1e2d54084cf99558160b5bebf439",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WwlrmyojrqbU"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDtGT5csi8XO"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mpGP8JU_r97w"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/lightning_logs/version_7/checkpoints/epoch=3-step=6436.ckpt\"\n",
        "\n",
        "embed_dim, context_length, vocab_size = 512, 256, 30002\n",
        "\n",
        "model = FNET.load_from_checkpoint(checkpoint_path, embed_dim=embed_dim, context_length=context_length, vocab_size=vocab_size, num_layers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c-gXr1_UjZrt"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "import json\n",
        "\n",
        "# Load pretrained tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"my_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "y9ShFsBTjjh4"
      },
      "outputs": [],
      "source": [
        "out = tokenizer.encode(\"\"\"Her first acting role of 2006 was in the comedy film The Pink Panther starring opposite Steve Martin, grossing $158.8 million at the box office worldwide. Her second film Dreamgirls, the film version of the 1981 Broadway musical loosely based on The Supremes, received acclaim from critics and grossed $154 million internationally. In it, she starred opposite Jennifer Hudson, Jamie Foxx, and Eddie Murphy playing a pop singer based on Diana Ross. To promote the film, Beyonc\u00e9 released \"Listen\" as the lead single from the soundtrack album. In April 2007, Beyonc\u00e9 embarked on The Beyonc\u00e9 Experience, her first worldwide concert tour, visiting 97 venues and grossed over $24 million.[note 1] Beyonc\u00e9 conducted pre-concert food donation drives during six major stops in conjunction with her pastor at St. John's and America's Second Harvest. At the same time, B'Day was re-released with five additional songs, including her duet with Shakira \"Beautiful Liar\".\n",
        "What movie did Beyonce act in 2006?\n",
        "The Pink Panther\n",
        "Her second movie Beyonce did was what film?\n",
        "Dreamgirls\n",
        "The single, \"Listen\" was featured in which movie?\n",
        "Dreamgirls\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fUNaWoCYj4D9"
      },
      "outputs": [],
      "source": [
        "inp = torch.tensor([out.ids+[0]*21]).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veSkNdioj6CX",
        "outputId": "9dfe054a-02b8-47cb-a82d-05ab7412b12b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 256])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BSYQpQorkf6J"
      },
      "outputs": [],
      "source": [
        "logits = model(inp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FQlEJwW2kzkL"
      },
      "outputs": [],
      "source": [
        "ids = torch.argmax(logits, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfURAGvIk6e2",
        "outputId": "7c89c9be-3e62-4334-ed4b-3cb6e14379c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ", the the the to of in the comedy the in in Pink opposite , , to , at Dreamgirls Dreamgirls Dreamgirls ? million featured the , . \" List List \" Dreamgirls Dreamgirls Dreamgirls Dreamgirls of the what the movie loosely musical on The The The The , 2006 in act Beyonce did \". grossed Li million Li million \" ira , it her opposite her including , songs , five x , - re was die ' a , pop based on At . . To . s film the Beyonc\u00e9 released ' released \" \" as with her single from the from ? . In April In - Beyonc\u00e9 - Beyonc\u00e9 Beyonc\u00e9 Beyonc\u00e9 Experience , Beyonc\u00e9 , million over the and tour 97 visiting 97 tour and the 24 million , Beyonc\u00e9 , note Beyonc\u00e9 Beyonc\u00e9 Beyonc\u00e9 Beyonc\u00e9 2007 - In April In . ? from single from single her with as \" \" released ' s Beyonc\u00e9 the film s To To . . At on based time , a ' Day was re - , x five , songs , including her opposite her it , ira \" million Li Beautiful Li grossed \". did Beyonce act in 2006 , The Pink The The second musical loosely movie the what the of Dreamgirls Dreamgirls Dreamgirls Dreamgirls \" List List \" . , the featured million ? Dreamgirls Dreamgirls Dreamgirls at , to , , , Pink in which the comedy the in of to the the the\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(ids.tolist()[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnopCQ9HlCtZ"
      },
      "outputs": [],
      "source": [
        "original_output_should_be = \"\"\"The single, \"Listen\" was featured in which movie?\n",
        "Dreamgirls\n",
        "Beyonce's first world tour was when?\n",
        "2007\n",
        "How much money did Beyonce's tour make in 2007?\n",
        "24 million\n",
        "How many millions of dollars did ''The Pink Panther'' gross world-wide?\n",
        "158.8 million\n",
        "What did Beyonce call her first concert tour?\n",
        "The Beyonc\u00e9 Experience\n",
        "Who was Beyonce's duet with in ''Beautiful Liar''?\n",
        "Shakira\n",
        "Which film did Beyonc\u00e9 star with Steve Martin in?\n",
        "The Pink Panther\n",
        "Beyonc\u00e9's role in Dreamgirls was based on what pop singer?\n",
        "Diana Ross.\n",
        "What was the lead single for the Dreamgirls soundtrack?\n",
        "Listen\n",
        "What was the name of Beyonc\u00e9's first international tour?\n",
        "The Beyonc\u00e9 Experience\n",
        "What pop singer did a duet with Beyonc\u00e9 on Beautiful Liar?\n",
        "Shakira\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "IcnsefiDlscR"
      },
      "outputs": [],
      "source": [
        "del model, output_tokens, logits, inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oKu1be0lxDa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}